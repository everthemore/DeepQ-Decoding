{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we begin by importing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evert/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "import gym\n",
    "\n",
    "from Function_Library import *\n",
    "from Environments import *\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../keras-rl')\n",
    "import rl as rl\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy, GreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of runs\n",
    "num_samples = 10\n",
    "# Code distance\n",
    "d = 5\n",
    "# Physical error rate\n",
    "p_phys = 0.007\n",
    "# Measurement error rate\n",
    "p_measurement_error = 0.007\n",
    "# Number of slices in syndrome volume\n",
    "num_slices = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadEnvAndAgent(d, p_phys, p_measurement_error):\n",
    "    fixed_configs_path = os.path.join(os.getcwd(),\"../trained_models/d{0}_x/fixed_config.p\".format(d))\n",
    "    variable_configs_path = os.path.join(os.getcwd(),\"../trained_models/d{0}_x/{1}/variable_config_77.p\".format(d,p_phys))\n",
    "    model_weights_path = os.path.join(os.getcwd(),\"../trained_models/d{0}_x/{1}/final_dqn_weights.h5f\".format(d,p_phys))\n",
    "\n",
    "    static_decoder_path = os.path.join(os.getcwd(),\"referee_decoders/nn_d5_X_p5\")\n",
    "    static_decoder = load_model(static_decoder_path)\n",
    "\n",
    "    fixed_configs = pickle.load( open(fixed_configs_path, \"rb\" ) )\n",
    "    variable_configs = pickle.load( open(variable_configs_path, \"rb\" ) )\n",
    "\n",
    "    all_configs = {}\n",
    "\n",
    "    for key in fixed_configs.keys():\n",
    "        all_configs[key] = fixed_configs[key]\n",
    "\n",
    "    for key in variable_configs.keys():\n",
    "        all_configs[key] = variable_configs[key]\n",
    "        \n",
    "    env = Surface_Code_Environment_Multi_Decoding_Cycles(d=all_configs[\"d\"], \n",
    "    p_phys=all_configs[\"p_phys\"], \n",
    "    p_meas=all_configs[\"p_meas\"],  \n",
    "    error_model=all_configs[\"error_model\"], \n",
    "    use_Y=all_configs[\"use_Y\"], \n",
    "    volume_depth=all_configs[\"volume_depth\"],\n",
    "    static_decoder=static_decoder)\n",
    "    \n",
    "    model = build_convolutional_nn(all_configs[\"c_layers\"],all_configs[\"ff_layers\"], \n",
    "                               env.observation_space.shape, env.num_actions)\n",
    "    memory = SequentialMemory(limit=all_configs[\"buffer_size\"], window_length=1)\n",
    "    policy = GreedyQPolicy(masked_greedy=True)\n",
    "    test_policy = GreedyQPolicy(masked_greedy=True)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "\n",
    "    dqn = DQNAgent(model=model, \n",
    "                   nb_actions=env.num_actions, \n",
    "                   memory=memory, \n",
    "                   nb_steps_warmup=all_configs[\"learning_starts\"], \n",
    "                   target_model_update=all_configs[\"target_network_update_freq\"], \n",
    "                   policy=policy,\n",
    "                   test_policy=test_policy,\n",
    "                   gamma = all_configs[\"gamma\"],\n",
    "                   enable_dueling_network=all_configs[\"dueling\"])  \n",
    "\n",
    "\n",
    "    dqn.compile(Adam(lr=all_configs[\"learning_rate\"]))\n",
    "    \n",
    "    dqn.model.load_weights(model_weights_path)\n",
    "    return env, dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../benchmark/faulty')\n",
    "\n",
    "import simulate_planar as sp\n",
    "import planar_lattice\n",
    "import perfect_matching\n",
    "\n",
    "def DecodeWithBlossom(faulty_syndrome_slices):\n",
    "   # The surface code lattice\n",
    "   L =planar_lattice.PlanarLattice(d)\n",
    "   # Parity lattice\n",
    "   PL=planar_lattice.PlanarLattice3D(d)\n",
    "\n",
    "   # For each time-slice\n",
    "   # Create corresponding L\n",
    "   for t in range(len(faulty_syndrome_slices)):\n",
    "\n",
    "      for x in range(d):\n",
    "          for y in range(d):\n",
    "              q0,q1 = L.positions_Q[x+y*d]\n",
    "\n",
    "              if( faulty_syndrome_slices[t][x,y] == 1 ):\n",
    "                  L.array[q0][q1][0] *= -1\n",
    "              if( faulty_syndrome_slices[t][x,y] == 2 ):\n",
    "                  L.array[q0][q1][0] *= -1\n",
    "                  L.array[q0][q1][1] *= -1\n",
    "              if( faulty_syndrome_slices[t][x,y] == 3 ):\n",
    "                  L.array[q0][q1][1] *= -1\n",
    "\n",
    "      PL.addMeasurement(L)\n",
    "\n",
    "    # Finally, add another layer with perfect measurements\n",
    "#   L.measurePlaquettes(0)\n",
    "#   L.measureStars(0)\n",
    "#   PL.addMeasurement(L)\n",
    "\n",
    "#   if showTextArray==True: L.showArrayText(\"errors\",\"X\")\n",
    "\n",
    "   # Find the errors\n",
    "   PL.findAnyons()\n",
    "\n",
    "   timespace=[1,1]\n",
    "   # Perfect matching on the plaquettes\n",
    "   matchingX = perfect_matching.match_planar_3D(d,\"plaquette\",PL.anyon_positions_P,timespace)\n",
    "   # Perfect matching on the starts\n",
    "   matchingZ = perfect_matching.match_planar_3D(d,\"star\",PL.anyon_positions_S,timespace)\n",
    "\n",
    "   # Reformat the matching\n",
    "   flipsX = sp.squashMatching(d,\"X\",matchingX)\n",
    "   flipsZ = sp.squashMatching(d,\"Z\",matchingZ)\n",
    "\n",
    "   # Apply the correction...\n",
    "   L.apply_flip_array(\"Z\",flipsZ)\n",
    "   L.apply_flip_array(\"X\",flipsX)\n",
    "\n",
    "   # ...and return the result of a logical error measurement\n",
    "   return L.measure_logical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecodeWithDeepQ(dqn, env, faulty_syndrome_slices):\n",
    "\n",
    "    # Intialize a zero'd input volume\n",
    "    input_state = np.zeros((d+1, 2*d + 1, 2*d + 1),int)\n",
    "\n",
    "    # embed and place the faulty syndrome slices in the correct place\n",
    "    for j in range(d):\n",
    "        input_state[j, :, :] = env.padding_syndrome(faulty_syndrome_slices[j])\n",
    "\n",
    "    corrections = []\n",
    "\n",
    "    still_decoding = True\n",
    "    while still_decoding:\n",
    "\n",
    "        # Fetch the suggested correction\n",
    "        action = dqn.forward(input_state)\n",
    "\n",
    "        if action not in corrections and action != env.identity_index:\n",
    "            # If the action has not yet been done, or is not the identity\n",
    "\n",
    "            # append the suggested correction to the list of corrections\n",
    "            corrections.append(action)\n",
    "\n",
    "            # Update the input state to the agent to indicate the correction it would have made\n",
    "            input_state[d, :, :] = env.padding_actions(corrections)\n",
    "\n",
    "        else:\n",
    "            # decoding should stop\n",
    "            still_decoding = False\n",
    "\n",
    "    # Now we'll have to perform these corrections, and check for a logical error\n",
    "    return corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_faulty_syndrome_slices():\n",
    "    \n",
    "    # Generate a new, clean lattice\n",
    "    qubits = generateSurfaceCodeLattice(d)\n",
    "    # And the hidden state\n",
    "    hidden_state = np.zeros((d, d), int)\n",
    "\n",
    "    faulty_syndrome_slices = []\n",
    "    for t in range(num_slices):\n",
    "        # Generate error pattern\n",
    "        error = generate_IIDXZ_error(d, p_phys)\n",
    "        #print(error)\n",
    "\n",
    "        # Update hidden state\n",
    "        hidden_state = obtain_new_error_configuration(hidden_state, error)\n",
    "\n",
    "        # Generate the true syndrome\n",
    "        true_syndrome = generate_surface_code_syndrome_NoFT_efficient(hidden_state, qubits)\n",
    "        #print(true_syndrome)\n",
    "\n",
    "        # Generate faulty syndrome\n",
    "        faulty_syndrome = generate_faulty_syndrome(true_syndrome, p_measurement_error)\n",
    "        #print(faulty_syndrome)\n",
    "\n",
    "        faulty_syndrome_slices.append(faulty_syndrome)\n",
    "    \n",
    "    return faulty_syndrome_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark():\n",
    "    # Set the random seed\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    success_count = 0\n",
    "    for i in range(num_samples):\n",
    "        faulty_syndrome_slices = generate_faulty_syndrome_slices()\n",
    "        \n",
    "        # At this point, we have a syndrome volume that we can attempt to decode\n",
    "\n",
    "        # Once with Blossom\n",
    "        x,z = DecodeWithBlossom(faulty_syndrome_slices)\n",
    "        if x==1 and z==1:\n",
    "            success_count+=1\n",
    "\n",
    "        # And then once with DeepQ\n",
    "        env, dqn = LoadEnvAndAgent(d, p_phys, p_measurement_error)\n",
    "        corrections = DecodeWithDeepQ(dqn, env, faulty_syndrome_slices)\n",
    "        print(corrections)\n",
    "\n",
    "    print(success_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 21]\n",
      "[]\n",
      "[]\n",
      "[12]\n",
      "[0]\n",
      "[14]\n",
      "[2]\n",
      "[21, 6, 11]\n",
      "[]\n",
      "[9, 3]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "benchmark();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
